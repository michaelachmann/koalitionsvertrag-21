{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auswertung Koalitionsvertrag\n",
    "## 1. Wort Frequenzen\n",
    "In diesem Notebook nutzen wir `NLTK` um die einzelnen Dokumente der Wahlprogramme und den Koalitionsvertrag einzulesen. Anschließend werden die Satzzeichen entfernt und die Dokumente tokenisiert. Die Tokens werden mit einer Stoppwort-Liste abgeglichen, um häufig vorkommende Wörter wie *ich, und, wir, ...* zu entfernen. Anschließend nutzen wir die `nltk.FreqDist()`-Funktion, um die Wortfrequenzlisten zu erstellen, die wir anschließend in einem `pandas.DataFrame()` speichern und zur weiteren Auswertung nutzen möchten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Stopwords from Github\n",
    "r = requests.get('https://github.com/stopwords-iso/stopwords-de/raw/master/stopwords-de.json')\n",
    "stop_words = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(filename, outputname):\n",
    "    # Opening File\n",
    "    with open(filename, 'r') as f:\n",
    "        corpus = f.read()\n",
    "        \n",
    "    # Tokenizing, removing punctuation\n",
    "    tokens = RegexpTokenizer(r'\\w+').tokenize(corpus)\n",
    "    \n",
    "    # Removing Stopwords\n",
    "    filtered_sentence = [w for w in tokens if not w.lower() in stop_words]\n",
    "    \n",
    "    # Creating Data Frame\n",
    "    df = pd.DataFrame.from_dict(nltk.FreqDist(filtered_sentence),orient='index')\n",
    "    df.columns = ['Frequency']\n",
    "    df.index.name = 'Term'\n",
    "    \n",
    "    # Exporting Data Frame\n",
    "    df.to_csv(outputname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define all documentnames, then we loop through them, creating a csv file for each document.\n",
    "files = [(\"data/fdp_b.txt\", \"fdp_freq.csv\"), (\"data/gruen_b.txt\", \"gruen_freq.csv\"), (\"data/spd_b.txt\", \"spd_freq.csv\"), (\"data/koav_b.txt\", \"koav_freq.csv\")]\n",
    "for file in files:\n",
    "    word_freq(file[0], file[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all Files to one large df (long vs wide)\n",
    "# This is an important step for using tidyr-packages later on\n",
    "def word_freq_long(filename, name):\n",
    "    # Opening File\n",
    "    with open(filename, 'r') as f:\n",
    "        corpus = f.read()\n",
    "        \n",
    "    # Tokenizing, removing punctuation\n",
    "    tokens = RegexpTokenizer(r'\\w+').tokenize(corpus)\n",
    "    \n",
    "    # Removing Stopwords\n",
    "    filtered_sentence = [w for w in tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = [w.lower() for w in filtered_sentence]\n",
    "    \n",
    "    # Creating Data Frame\n",
    "    df = pd.DataFrame.from_dict(nltk.FreqDist(filtered_sentence),orient='index')\n",
    "    df.columns = ['Frequency']\n",
    "    df.index.name = 'Term'\n",
    "    df['Source'] = name\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all textfiles, creating one df for all frequency lists\n",
    "files = [(\"data/fdp_b.txt\", \"FDP\"), (\"data/gruen_b.txt\", \"GRUENE\"), (\"data/spd_b.txt\", \"SPD\"), (\"data/koav_b.txt\", \"KOALITION\")]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for file in files:\n",
    "    _df = word_freq_long(file[0], file[1])\n",
    "    df = df.append(_df)\n",
    "    \n",
    "# Export to CSV\n",
    "df.to_csv(\"all_freq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrl}\n",
      "\\toprule\n",
      "{} &  Frequency &     Source \\\\\n",
      "Term         &            &            \\\\\n",
      "\\midrule\n",
      "innen        &        451 &     GRUENE \\\\\n",
      "freie        &        386 &        FDP \\\\\n",
      "demokraten   &        375 &        FDP \\\\\n",
      "stärken      &        238 &  KOALITION \\\\\n",
      "eu           &        181 &     GRUENE \\\\\n",
      "stärken      &        181 &     GRUENE \\\\\n",
      "unterstützen &        166 &  KOALITION \\\\\n",
      "setzen       &        156 &  KOALITION \\\\\n",
      "eu           &        148 &  KOALITION \\\\\n",
      "innen        &        136 &        SPD \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the top 10 terms as a LaTeX Table for use in the presentation\n",
    "print(df.sort_values(\"Frequency\", ascending=False)[0:10].to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Relative Wortfrequenzen\n",
    "Obige Ergebnisse sind noch nicht sehr aussagekräftig, da sie nur absolute Zahlen wiedergeben. Nun sind die Wahlprogramme allerdings unterschiedlich lang, deshalb wäre es spannend die Wortfrequenz in Bezug zur Gesamtsumme zu sehen, wir wollen also berechnen: $\\frac{Frequency}{Total Words}=Relative Frequency \\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_relative_freq_long(filename, name):\n",
    "    # Opening File\n",
    "    with open(filename, 'r') as f:\n",
    "        corpus = f.read()\n",
    "        \n",
    "    # Tokenizing, removing punctuation\n",
    "    tokens = RegexpTokenizer(r'\\w+').tokenize(corpus)\n",
    "    \n",
    "    # Removing Stopwords\n",
    "    filtered_sentence = [w for w in tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = [w.lower() for w in filtered_sentence]\n",
    "    \n",
    "    # Creating Data Frame\n",
    "    freq = nltk.FreqDist(filtered_sentence)\n",
    "    total_tokens = sum(freq.values()) # Here we calculate the total number of tokens in our Frequency List\n",
    "    df = pd.DataFrame.from_dict(freq,orient='index')\n",
    "    df.columns = ['Frequency']\n",
    "    df.index.name = 'Term'\n",
    "    df['Source'] = name\n",
    "    df['Relative'] = (df['Frequency'] / total_tokens) * 100 # Here we add a new column `relative` (*100 for percentage)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all textfiles, creating one df for all frequency lists\n",
    "files = [(\"data/fdp_b.txt\", \"FDP\"), (\"data/gruen_b.txt\", \"GRUENE\"), (\"data/spd_b.txt\", \"SPD\"), (\"data/koav_b.txt\", \"KOALITION\")]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for file in files:\n",
    "    _df = word_relative_freq_long(file[0], file[1])\n",
    "    df = df.append(_df)\n",
    "    \n",
    "# Export to CSV\n",
    "df.to_csv(\"all_freq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrlr}\n",
      "\\toprule\n",
      "{} &  Frequency &     Source &  Relative \\\\\n",
      "Term         &            &            &           \\\\\n",
      "\\midrule\n",
      "freie        &        386 &        FDP &  2.070593 \\\\\n",
      "demokraten   &        375 &        FDP &  2.011587 \\\\\n",
      "innen        &        451 &     GRUENE &  1.315981 \\\\\n",
      "innen        &        136 &        SPD &  1.169088 \\\\\n",
      "stärken      &        238 &  KOALITION &  0.879657 \\\\\n",
      "eu           &        128 &        FDP &  0.686622 \\\\\n",
      "unterstützen &        166 &  KOALITION &  0.613542 \\\\\n",
      "fordern      &        110 &        FDP &  0.590065 \\\\\n",
      "setzen       &        156 &  KOALITION &  0.576582 \\\\\n",
      "deutschland  &        106 &        FDP &  0.568609 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the top 10 terms as a LaTeX Table for use in the presentation\n",
    "print(df.sort_values(\"Relative\", ascending=False)[0:10].to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
